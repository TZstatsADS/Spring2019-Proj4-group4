---
title: 'Optical character recognition (OCR)'
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
    code_folding: hide
---

Group 4

GU4243/GR5243: Applied Data Science

<style type="text/css">
h1.title {
  font-size: 24px;
  color: Black;
}
h1 { /* Header 1 */
  font-size: 24px;
  color: Black;
}
h2 { /* Header 2 */
  font-size: 20px;
  color: Black;
}
h3 { /* Header 3 */
  font-size: 16px;
  color: Black;
}
h4 { /* Header 4 */
  font-size: 14px;
  color: Grey;
}
</style>
# Introduction {-}

Optical character recognition (OCR) is the process of converting scanned images of machine printed or
handwritten text (numerals, letters, and symbols), into machine readable character streams, plain (e.g. text files) or formatted (e.g. HTML files). As shown in Figure 1, the data *workflow* in a typical OCR system consists of three major stages:

* Pre-processing

* OCR character recognition

* Post-processing

![](../figs/ocr_flowchart.png) 

We have processed raw scanned images through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

BUT this is not the FOCUS of this project!!!

In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  

We have processed raw scanned images through the first two steps are relying on the [Tessearct OCR machine](https://en.wikipedia.org/wiki/Tesseract_(software)). R package tutorial can be found [here](https://www.r-bloggers.com/the-new-tesseract-package-high-quality-ocr-in-r/). 

BUT this is not the FOCUS of this project!!!

In this project, we are going to **focus on the third stage -- post-processing**, which includes two tasks: *error detection* and *error correction*.  
##Step 1 - Load library and source code
```{r, warning=FALSE, message = FALSE}
if (!require("devtools")) install.packages("devtools")
if (!require("pacman")) {
  ## devtools is required
  library(devtools)
  install_github("trinker/pacman")
}

library(tm)
library(dplyr)
library(stringdist)
library(e1071)
library(foreach)
library(doParallel)
library(parallelSVM)
library(tidytext)
library(tidyverse)
library(DT)
library(topicmodels)
library(tidyr)
library(readr)
pacman::p_load(knitr, readr, stringr, tesseract, vecsets)
source('../lib/ifCleanToken.R')
true_files <- list.files("../data/ground_truth") #100 files in total
ocr_files <- list.files("../data/tesseract")
options(warn=-1)
file_name_vec <- list.files("../data/ground_truth") #100 files in total
```

# Error detection
## Step1 Use train data to create LB corpus
```{r ,warning=FALSE, message = FALSE}
set.seed(2019)
train_index <- sample(1:100, 80, replace = F)
# bigr_list: a list of 80 file names
bigr_list <- file_name_vec[train_index]
read_txt <- function(file_name){
  current_file_name <- sub(".txt","",file_name)
  current_ground_truth <- readLines(paste("../data/ground_truth/",current_file_name,".txt",sep=""), encoding="UTF-8",warn=FALSE)
  return(current_ground_truth)
}

bigr_lib <- lapply(bigr_list, read_txt)
  
corpus<-VCorpus(VectorSource(bigr_lib))%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords, character(0))%>%
    tm_map(stripWhitespace)
dict <- tidy(corpus) %>%
  select(text)  
data("stop_words")
completed <- dict %>%
  mutate(id = bigr_list)  %>%
  unnest_tokens(dictionary, text) %>%
  anti_join(stop_words,by = c("dictionary" = "word")) 
list <- completed$dictionary
source('../lib/feature_extraction.R')
Lb <- unlist(lapply(list,bigram)) #778458 elements
LB <- data.frame(table(Lb))
write.csv(LB, file = "../output/LB.csv")
```

## Step2: Obtain ocr data and extract the features
```{r ,warning=FALSE, message = FALSE}
tokens <- read.csv("../output/processed_tokens.csv")
#LB <- read.csv("../output/LB.csv")

feature_names <- paste("feature", 1:16)

feature_2 <- matrix(nrow = nrow(tokens), ncol = 3)
feature_5 <- matrix(nrow = nrow(tokens), ncol = 2)
feature_1 <- feature_3 <- feature_4 <- feature_6 <-feature_7<- NULL
feature_8 <- feature_9 <- feature_10 <-feature_11 <- feature_12 <- feature_13 <- NULL
for(i in 1:nrow(tokens)){
  feature_1[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat1()
  feature_2[i,] <- tokens$ocr_tokens[i] %>% as.character() %>% feat2() 
  feature_3[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat3()
  feature_4[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat4()
  feature_5[i,] <- tokens$ocr_tokens[i] %>% as.character() %>% feat5() 
  feature_6[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat6() 
  feature_7[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat7() 
  feature_8[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat8() 
  feature_9[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat9() 
  feature_10[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat10()
  feature_11[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat11()
  feature_12[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat12()
  feature_13[i] <- tokens$ocr_tokens[i] %>% as.character() %>% feat13()
} 

features <- cbind(feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9, feature_10,feature_11, feature_12, feature_13)
colnames(features) <- feature_names
features <- cbind(tokens, features)
save(features, file="../output/features.RData")
write.csv(features, file="../output/features.csv", row.names = FALSE)
# load("../output/features.RData")
```

#Error Correction Part

##Step 2 - Read ground_truth and tesseract file 
```{r ,warning=FALSE, message = FALSE}
#function to read in ground_truth data
read_truth <- function(file_name){
  current_name <- sub(".txt","",file_name)
  ground_truth <- readLines(paste("../data/ground_truth/",current_name,".txt",sep=""), encoding="UTF-8",warn=FALSE)
  return(ground_truth)
}

#function to read in ocr data
read_ocr <- function(file_name){
  current_name <- sub(".txt","",file_name)
  current_ocr <- readLines(paste("../data/tesseract/",current_name,".txt",sep=""), encoding="UTF-8",warn=FALSE)
  return(current_ocr)
}

##read the ground_truth 
truth_list <- lapply(true_files,read_truth)

##read the tesseract
ocr_list <- lapply(ocr_files, read_ocr)

#function for combining sentence pieces together
combining <- function(txt){
  whole_text=paste(txt,collapse=" ")
  return(whole_text)
}
```

##Step 3 - Read in the token list and feature list
```{r, message=FALSE, warning=FALSE}
Features <- read_csv("../output/features.csv") #read in the features outcome from detection
#data <- read_csv("../output/svm_output.csv")
svm_output <-read_csv("../output/svm_output.csv") #read in svm outcome
data_final <- cbind(svm_output,Features)
data_final <- data_final[,-(6:9)] #combine to one whole dataframe and eliminate the duplicates

svm_output$doc_id<-as.character(svm_output$doc_id)
data_final$doc_id <- as.character(data_final$doc_id) #change type of doc_id to character
#head(data_final)

```

##Step 4 - Output the detected error for correction
```{r,  ,warning=FALSE, message = FALSE}
#data<-read_csv("../output/SVM_output.csv")
test_list=sample(true_files,floor(length(true_files)*0.25))
train_list=true_files
location <- which(true_files %in% train_list)
test_loc <- which(true_files %in% test_list)
#train_data <- data_final

data_test <- data_final[(data_final[,"doc_id"] %in%(test_loc)),]

pred_all<-data_final$predicted #predicted results
#data_error_test <- data.frame(data_test,p=pred_all[(data_final[,"doc_id"] %in% test_loc)])
data_error_test <- data_test

error <- data_final[data_final$predicted ==1,]
all_error_ind <- which(data_final$predicted==1)
test_error_ind <- which(data_test$predicted==1)

non_error <- data_final[(data_final$predicted==0),]
error_list <- list()
index_list <- list()
index_list_test <- list()
error_list_test <- list()
#get a list of all error tokens and a list of their index
for(i in 1:100){
  error_list[[i]] <- as.character(error[error$doc_id==i,"ocr_tokens"])
  index_list[[i]] <- all_error_ind[error$doc_id==i]
}

#get a list of test error tokens and a list of their index
for(i in 1:25){
  error_list_test[[i]] <- as.character(error[error$doc_id==test_loc[i],"ocr_tokens"])
  index_list_test[[i]] <- test_error_ind[error$doc_id==test_loc[i]]
}

p1 <- "[[:punct:]]"
p2 <- "[A-Z]"
p3 <- "[0-9]"
lo <- list()
lo2 <- list()
error_uncleaned_test <- list()
error_uncleaned <- list()
error_cleaned_test <- list()
error_cleaned <- list()
ground_file <- list()
non_error_test_ocr <- list()
f <- function(vector){
  logic <- rep(FALSE,length(vector))
  for(i in 1:length(vector)){
    word <- vector[i]
    s <- unlist(strsplit(word,split=""))
    l1 <- grepl(p1,s)
    l2 <- grepl(p2,s)
    l3 <- grepl(p3,s)
    l4 <- ifelse(nchar(word)==1,1,0)
    if(sum(l1,l2,l3,l4)==0){
      logic[i] = TRUE
    }
  }
  return(logic) 
}
index_unselected <- list()
index_unselected_test <- list()
index_selected <- list()
index_selected_test <- list()

for(i in 1:100){
  lo[[i]] <- f((error_list[[i]]))
  error_cleaned[[i]] <- as.character(error_list[[i]][(lo[[i]])==TRUE])
  error_uncleaned[[i]] <- as.character(error_list[[i]][(lo[[i]])==FALSE])
  index_selected[[i]] <- index_list[[i]][(lo[[i]]==TRUE)]
  index_unselected[[i]] <- index_list[[i]][(lo[[i]]==FALSE)]
 
}

for(k in 1:25){
  i<-test_loc[k]
  error_cleaned_test[[k]] <- error_cleaned[[i]]
  error_uncleaned_test[[k]] <- error_uncleaned[[i]]
  index_selected_test[[k]] <- index_selected[[i]]
  index_unselected_test[[k]] <- index_unselected[[i]]

}
index <- list()
ground_truth_selected <- list()
ocr_selected <- list()
for(i in 1:100){
  number <- min(length(truth_list[[i]]),length(ocr_list[[i]]))
  length_ground <- rep(NA,number)
  length_ocr <- rep(NA,number)
  for(j in 1:number){
    s_ground  <- unlist(strsplit((truth_list[[i]])[j],split=" "))
    s_truth <- unlist(strsplit((ocr_list[[i]])[j],split=" "))
    length_ground[j] <- length(s_ground)
    length_ocr[j] <- length(s_truth)
  }
  index[[i]]<- which(length_ground==length_ocr)
}
for( i in 1:100){
  ground_truth_selected[[i]] <- truth_list[[i]][index[[i]]]
  ocr_selected[[i]] <- ocr_list[[i]][index[[i]]]
}

ground_truth_selected_test <- ground_truth_selected[test_loc]
ground_truth_selected_test_noerror <- ground_truth_selected_test
ocr_selected_test <- ocr_selected[test_loc]

ground_file <- lapply(ground_truth_selected,combining)
ground_file_test <- lapply(ground_truth_selected_test,combining)
ocr_file_test <- lapply(ocr_selected_test,combining)
```


##Step 5 - Remove numbers, punctuations,stopwords etc 
```{r ,warning=FALSE, message = FALSE}

lapply(paste("../data/ground_truth/",true_files,sep=""),readLines,encoding="UTF-8")->truth_list


getWholeTxt=function(piece_txt)
{
whole_txt=paste(piece_txt,collapse=" ")
return(whole_txt)
}


lapply(truth_list,getWholeTxt)->truth_word_list


#### remove numbers, punctuations etc 

truth_corpus<-VCorpus(VectorSource(truth_word_list))%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords, character(0))%>%
    tm_map(stripWhitespace)

dict <- tidy(truth_corpus) %>%
  select(text)  


#### remove stopwords
data("stop_words")

completed <- dict %>%
  mutate(id = true_files)  %>%
  unnest_tokens(dictionary, text) %>%
  anti_join(stop_words,by = c("dictionary" = "word")) 


bag_of_words <- completed%>%
  select(dictionary)%>%
  distinct()

#nrow(bag_of_words) #13593 bags of words 


completed1 <- completed %>%
  group_by(id) %>%
  summarise(text = str_c(dictionary, collapse = " ")) %>%
  ungroup()
```

##Step 6 -Training data for LDA
```{r ,warning=FALSE, message = FALSE}
#### get a freq matrix
VCorpus(VectorSource(completed1$text))->cleaned_corpus
DocumentTermMatrix(cleaned_corpus)->dtm
#### set parameters and run
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k=4

LDA(dtm,k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))->mod_4
```

```{r}
topicProbabilities <- as.data.frame(mod_4@gamma) 
post_topics<- matrix(0,25,4)
for (i in 1:25){
  post_topics[i,] =  as.numeric(topicProbabilities[test_loc[i],])
  
}

#post_topics 
```

##Step 7 - Get dictionary, dtm_test, dictionary_stop
```{r ,warning=FALSE, message = FALSE}
dictionary=dtm$dimnames[[2]]

dictionary_stop=c(dictionary,stop_words$word)
```

##Step 8 - Confusion Matrice and Pobability Pr(ljf|ljs) 
```{r ,warning=FALSE, message = FALSE}
hhh<-read.csv('../data/sub_matrix.csv')
hhh <- hhh[,2:27] 
 sub<- matrix(as.numeric( unlist(matrix(hhh,26,26))),26,26)
 
rownames(sub)<-c("a","b","c","d","e","f","g","h","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z")
colnames(sub)<-c("a","b","c","d","e","f","g","h","i","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z")

prob<-sub/colSums(sub)
```

###Step 9 - Replace word error
```{r ,warning=FALSE, message = FALSE}
# source("../lib/Functions.R")
# replace_all_file(error_cleaned_test)->correction_list
# 
# save(correction_list,file="../output/correction_list_updated.RData")
```


```{r}
source("../lib/Functions.R")
load("../output/correction_list_updated.RData")

correction_list_updated<-correction_list

for(i in 1:25){
  l = correction_list[[i]]
  for(j in 1:length(l)){
    if(identical(l[[j]],character(0))){
      correction_list[[i]][[j]] <- names(correction_list[[i]][j])
    }
  }
}
corrections <- as.character(unlist(correction_list))

ground_truth_selected_test <- lapply(ground_truth_selected_test,combining)
ground_file_test_corpus<-VCorpus(VectorSource(ground_truth_selected_test))
ground_file_test <- tidy(ground_file_test_corpus) %>%
  select(text)%>%
  unnest_tokens(dictionary, text) 

corrected_file_test=ground_file_test$dictionary
truth <- corrected_file_test[unlist(index_selected_test)]
corrected_file_test[unlist(index_selected_test)] <- corrections



```

##Step 10 - Performance evaluation


```{r ,warning=FALSE, message = FALSE}
####################### Performance for Tesseract with Postprocessing ##############################
ground_truth_selected_test_corpus<-VCorpus(VectorSource(ground_truth_selected_test))
ground_truth_selected_test <- tidy(ground_truth_selected_test_corpus) %>%
  select(text)%>%
  unnest_tokens(dictionary, text) 
ground_truth_selected_test <- ground_truth_selected_test$dictionary

#### wordwise recall
sum1<-0
for (i in 1:length(ground_truth_selected_test)){
  if(ground_truth_selected_test[i]==corrected_file_test[i]){
  sum1 = sum1+1
  }
}
wr<-sum1 / length(ground_truth_selected_test)

#### wordwise precision
ocr_selected_test_corpus<-VCorpus(VectorSource(ocr_selected_test))
ocr_selected_test <- tidy(ocr_selected_test_corpus) %>%
  select(text)%>%
  unnest_tokens(dictionary, text) 
ocr_selected_test$dictionary->ocr_selected_test

sum2<-0
for (i in 1:length(ground_truth_selected_test)){
  if(ground_truth_selected_test[i]==corrected_file_test[i]){
  sum2 = sum2+1
  }
}
wp<-sum2 / length(ocr_selected_test)
#### characterwise recall
corrected_file_test = corrected_file_test[1:length(ground_truth_selected_test)]
index_non <- which(unlist(lapply(ground_truth_selected_test,nchar))!=unlist(lapply(corrected_file_test,nchar)))

list_ground <- unlist(lapply(ground_truth_selected_test,nchar))
list_corrected <- unlist(lapply(corrected_file_test,nchar))
for(i in (index_non)){
  if(list_ground[i] > list_corrected[i]){
    s = unlist(strsplit(ground_truth_selected_test[i],split=""))
    diff = list_ground[i]-list_corrected[i]
    ground_truth_selected_test[i] = paste(s[1:(list_ground[i]-diff)],collapse = "")
  }
  else{
    s = unlist(strsplit(corrected_file_test[i],split=""))
    diff = list_corrected[i]-list_ground[i]
    corrected_file_test[i] = paste(s[1:(list_corrected[i]-diff)],collapse = "")
  }
}
ground_truth_character=strsplit(ground_truth_selected_test,integer(0))%>%unlist()
corrected_character=strsplit(corrected_file_test,integer(0))%>%unlist()

mean(ground_truth_character==corrected_character)->cr

#### characterwise precision
ocr_test_character=strsplit(ocr_selected_test,integer(0))%>%unlist()
sum(ground_truth_character==corrected_character)/length(ocr_test_character)->cp
#save(ocr_test_character,file="../output/ocr_test_character.RData")

```

```{r ,warning=FALSE, message = FALSE}
################################### Performance for Tesseract ######################################
load("../output/ocr_test_character.RData")
### wordwise recall
wr_tess <- mean(as.character(data_test$correct_tokens)==as.character(data_test$ocr_tokens))
### wordwise precision
wp_tess <- sum(as.character(data_test$correct_tokens)==as.character(data_test$ocr_tokens))/length(ocr_selected_test)

### characterwise recall
index_non_ocr <- which(nchar(as.character(data_test$correct_tokens))!=nchar(as.character(data_test$ocr_tokens)))
data_test_truth <- as.character(data_test$correct_tokens)
data_test_ocr <- as.character(data_test$ocr_tokens)                                    
list_corrected_tess <- unlist(lapply(data_test_ocr,nchar))
list_ground_tess <- unlist(lapply(data_test_truth,nchar))

for(i in (index_non_ocr)){
  if(list_ground_tess[i] > list_corrected_tess[i]){
    s = unlist(strsplit(data_test_truth[i],split=""))
    diff = list_ground_tess[i]-list_corrected_tess[i]
    data_test_truth[i] = paste(s[1:(list_ground_tess[i]-diff)],collapse = "")
  }
  else{
    s = unlist(strsplit(data_test_ocr[i],split=""))
    diff = list_corrected_tess[i]-list_ground_tess[i]
    data_test_ocr[i] = paste(s[1:(list_corrected_tess[i]-diff)],collapse = "")
  }
}

data_error_test_s <- strsplit(data_test_truth,integer(0))%>%unlist()
data_error_test_o <- strsplit(data_test_ocr,integer(0))%>%unlist()

cr_tess <-  mean(data_error_test_s==data_error_test_o)

### characterwise precision
cp_tess <- sum(data_error_test_s==data_error_test_o)/length(ocr_test_character)

OCR_performance_table <- data.frame("Tesseract" = rep(NA,4),
                                    "Tesseract_with_postprocessing" = rep(NA,4))
row.names(OCR_performance_table) <- c("word_wise_recall","word_wise_precision","character_wise_recall","character_wise_precision")
OCR_performance_table["word_wise_recall","Tesseract"] <- wr_tess
OCR_performance_table["word_wise_precision","Tesseract"] <-  wp_tess
OCR_performance_table["character_wise_recall","Tesseract"] <- cr_tess
OCR_performance_table["character_wise_precision","Tesseract"] <-  cp_tess
OCR_performance_table["word_wise_recall","Tesseract_with_postprocessing"] <- wr
OCR_performance_table["word_wise_precision","Tesseract_with_postprocessing"] <- wp 
OCR_performance_table["character_wise_recall","Tesseract_with_postprocessing"] <- cr
OCR_performance_table["character_wise_precision","Tesseract_with_postprocessing"] <- cp 
kable(OCR_performance_table, caption="Summary of OCR performance")
```
